# Default values for the helm chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
#
# Note about Namespace
# --------------------
# It is deliberately left out here and using the helm -n or --namespace flag you can deploy your resources to the same
# namespace as the release. If you leave it out, your resources will be deployed to the default namespace.
# Also, not that the namespace you are deploying to should already exist otherwise the helm command will fail.
# You can always specify a different namespace for a resource by setting it directly in it's yaml file or
# making it configurable by defining it in this file.

###########
# Deployment and Service
###########
replicaCount: 1
maxUnavailable: 0

image:
  repository: traceableai-docker.jfrog.io/hypertrace/hypertrace-data-config-service
  pullPolicy: IfNotPresent
  tagOverride: "test"

imagePullSecrets: {}

containerPort: 9012
containerAdminPort: 9013

service:
  name: hypertrace-data-config-service
  type: ClusterIP
  port: 9012

nodeLabels: {}

javaOpts: "-XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=75.0"

livenessProbe:
  initialDelaySeconds: 10
  periodSeconds: 5

readinessProbe:
  initialDelaySeconds: 2
  periodSeconds: 5

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  requests:
    cpu: 0.1
    memory: 1024Mi
  limits:
    cpu: 1
    memory: 1024Mi

deploymentLabels:
  app: hypertrace-data-config-service

podLabels:
  app: hypertrace-data-config-service

podAnnotations: {}

# The Deployment Selector match labels are different from the pod labels. Note that they should be a subset of the pod
# labels. You append new labels to them but cannot remove labels. If you remove or modify the labels you will need to
# delete the existing deployment bearing the same name and then redeploy. This is the reason why they are separated from
# the pod labels. You can add and remove pod labels without having an effect on the deployment.
# Also, please use "apiVersion: apps/v1" instead of the deprecated "apiVersion: extensions/v1beta1" for the deployment
# apiVersion in the yaml file.
deploymentSelectorMatchLabels:
  app: hypertrace-data-config-service

serviceSelectorLabels:
  app: hypertrace-data-config-service

###########
# Config Maps
###########
attributeServiceConfig:
  name: attribute-service-config
  dataStoreType: "mongo"
  mongo:
    host: mongo
    url: ""
  postgres:
    host: postgres
    port: 5432
    url: ""

configServiceConfig:
  name: config-service-config
  dataStoreType: "mongo"
  mongo:
    host: mongo
    url: ""

entityServiceConfig:
  name: entity-service-config
  dataStoreType: "mongo"
  mongo:
    host: mongo
    url: ""
  postgres:
    host: postgres
    port: 5432
    url: ""

hypertraceDataConfigServiceConfig:
  name: hypertrace-data-config-service-config
  data:
    application.conf: |-
      service.port = 9012
      service.admin.port = 9013

logConfig:
  name: hypertrace-data-config-service-log-config
  monitorInterval: 30
  rootLogger:
    level: INFO
  appender:
    rolling:
      enabled: false

config-bootstrapper:
  jobs:
    attribute:
      prefix: attribute
      configurationCommands: {}
    entity:
      prefix: entity
      configurationCommands: {}
  entityServiceConfig:
    name: entity-service-config
    data:
      host: hypertrace-data-config-service
      port: 9012
  attributeServiceConfig:
    name: attribute-service-config
    data:
      host: hypertrace-data-config-service
      port: 9012
  dataStoreType: "mongo"
  mongoConfig:
    name: mongo-config
    data:
      host: mongo
      port: 27017
      url: ""
